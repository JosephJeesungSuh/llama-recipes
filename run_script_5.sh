export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=4 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'cregion' --custom_dataset_leave_wave_34_testing_one_group.group 'northeast' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_cregion_northeast --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_cregion_northeast --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=4 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'cregion' --custom_dataset_leave_wave_34_testing_one_group.group 'south' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_cregion_south --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_cregion_south --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=4 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'poliedology' --custom_dataset_leave_wave_34_testing_one_group.group 'conservative' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_poliedology_conservative --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_poliedology_conservative --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=4 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'poliedology' --custom_dataset_leave_wave_34_testing_one_group.group 'liberal' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_poliedology_liberal --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_poliedology_liberal --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=4 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'poliedology' --custom_dataset_leave_wave_34_testing_one_group.group 'moderate' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_poliedology_moderate --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_poliedology_moderate --lr 4e-4 --num_epochs 20 --loss_function_type wd

export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=5 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'relig' --custom_dataset_leave_wave_34_testing_one_group.group 'protestant' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_protestant --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_protestant --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=5 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'relig' --custom_dataset_leave_wave_34_testing_one_group.group 'atheist' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_atheist --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_atheist --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=5 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'relig' --custom_dataset_leave_wave_34_testing_one_group.group 'muslim' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_muslim --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_muslim --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=5 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'relig' --custom_dataset_leave_wave_34_testing_one_group.group 'hindu' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_hindu --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_hindu --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=5 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'relig' --custom_dataset_leave_wave_34_testing_one_group.group 'jewish' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_jewish --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_relig_jewish --lr 4e-4 --num_epochs 20 --loss_function_type wd

export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=6 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'polparty' --custom_dataset_leave_wave_34_testing_one_group.group 'democrat' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_polparty_democrat --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_polparty_democrat --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=6 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'polparty' --custom_dataset_leave_wave_34_testing_one_group.group 'republican' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_polparty_republican --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_polparty_republican --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=6 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'income' --custom_dataset_leave_wave_34_testing_one_group.group 'less_than_$30,000' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_income_less_than_$30,000 --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_income_less_than_$30,000 --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=6 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'income' --custom_dataset_leave_wave_34_testing_one_group.group '$100,000_or_more' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_income_$100,000_or_more --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_income_$100,000_or_more --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=6 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'sex' --custom_dataset_leave_wave_34_testing_one_group.group 'male' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_sex_male --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_sex_male --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=6 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'sex' --custom_dataset_leave_wave_34_testing_one_group.group 'female' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_sex_female --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_sex_female --lr 4e-4 --num_epochs 20 --loss_function_type wd

export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=7 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'race' --custom_dataset_leave_wave_34_testing_one_group.group 'white' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_race_white --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_race_white --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=7 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'race' --custom_dataset_leave_wave_34_testing_one_group.group 'black' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_race_black --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_race_black --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=7 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'race' --custom_dataset_leave_wave_34_testing_one_group.group 'asian' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_race_asian --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_race_asian --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=7 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'race' --custom_dataset_leave_wave_34_testing_one_group.group 'hispanic' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_race_hispanic --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_race_hispanic --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=7 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'education' --custom_dataset_leave_wave_34_testing_one_group.group 'less_than_high_school' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_education_less_than_high_school --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_education_less_than_high_school --lr 4e-4 --num_epochs 20 --loss_function_type wd
export HF_TOKEN=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 && cd ~/fork/llama-recipes/ && CUDA_VISIBLE_DEVICES=7 torchrun --nnodes=1 --nproc-per-node=1 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 16 --val_batch_size 16 --gradient_accumulation_steps 1 --gamma 0.9 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_one_group' --custom_dataset_leave_wave_34_testing_one_group.attribute 'education' --custom_dataset_leave_wave_34_testing_one_group.group 'college_graduate_some_postgrad' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_education_college_graduate_some_postgrad --name 7_1_2_split_wdloss_testing_lr_4e-4_leave_wave_34_out_one_group_education_college_graduate_some_postgrad --lr 4e-4 --num_epochs 20 --loss_function_type wd