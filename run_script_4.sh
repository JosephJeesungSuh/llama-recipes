export HF_TOKEN=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 && cd ~/llama-recipes/ && CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun --nnodes=1 --nproc-per-node=4 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 32 --val_batch_size 64 --gradient_accumulation_steps 4 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_all_steering_method' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_celoss_testing_lr_1e-4_leave_wave_34_out_all_steering_method --name 7_1_2_split_celoss_testing_lr_1e-4_leave_wave_34_out_all_steering_method --lr 1e-4 --num_epochs 20 --loss_function_type ce

export HF_TOKEN=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 && cd ~/llama-recipes/ && CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun --nnodes=1 --nproc-per-node=4 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 32 --val_batch_size 64 --gradient_accumulation_steps 4 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_all_steering_method' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_celoss_testing_lr_4e-4_leave_wave_34_out_all_steering_method --name 7_1_2_split_celoss_testing_lr_4e-4_leave_wave_34_out_all_steering_method --lr 4e-4 --num_epochs 20 --loss_function_type ce

export HF_TOKEN=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 && cd ~/llama-recipes/ && CUDA_VISIBLE_DEVICES=4,5,6,7 torchrun --nnodes=1 --nproc-per-node=4 --master_port=29501 recipes/quickstart/finetuning/finetuning.py --enable_fsdp --low_cpu_fsdp --fsdp_config.pure_bf16 --use_peft --peft_method='lora' --use_fp16 --mixed_precision --batch_size_training 32 --val_batch_size 64 --gradient_accumulation_steps 4 --dist_checkpoint_root_folder ~/llama-recipes/outputs/llama2/ --dist_checkpoint_folder llama2 --dataset 'custom_dataset_leave_wave_34_testing_all_steering_method' --batching_strategy='padding' --output_dir /rscratch/data/steerable_pluralism/lora/7_1_2_split_celoss_testing_lr_1e-3_leave_wave_34_out_all_steering_method --name 7_1_2_split_celoss_testing_lr_1e-3_leave_wave_34_out_all_steering_method --lr 1e-3 --num_epochs 20 --loss_function_type ce
